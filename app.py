import gradio as gr
import os
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments
from huggingface_hub import login
from dotenv import load_dotenv
# from huggingface_hub import notebook_login

load_dotenv()
#  token = os.environ['sorsw']
#  token = 'token and must add variable'; 

token = os.getenv('sors')
login(token)
# notebook_login()

dataset = load_dataset("meta-llama/Meta-Llama-3.1-8B-Instruct-evals", "Meta-Llama-3.1-8B-Instruct-evals__arc_challenge__details", use_auth_token=True)
 
model_name = "meta-llama/Meta-Llama-3.1-8B-Instruct"
model = AutoModelForCausalLM.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)


def train_model(epochs=3):
    training_args = TrainingArguments(
        output_dir="output",  # Adjust output directory
        per_device_train_batch_size=8,  # Adjust batch size
        num_train_epochs=epochs,
        evaluation_strategy="epoch",  # Adjust evaluation strategy
    )

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=dataset,
    )

    trainer.train()
    print("Model training complete!")


def generate_text(prompt):
    try:
        input_ids = tokenizer(prompt, return_tensors="pt").input_ids
        output = model.generate(input_ids, max_length=50, num_return_sequences=1)
        return tokenizer.decode(output[0], skip_special_tokens=True)
    except Exception as e:
        return f"Error generating text: {e}"


interface = gr.Interface(
    fn=generate_text,
    inputs="text",
    outputs="text",
    title="Text Generation with Trained Model",
    description="Enter a prompt and get creative text generated by the model.",
)


train_model()  
interface.launch()




